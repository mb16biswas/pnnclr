{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faJB-7fg0xho"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/MyDrive/self_supervised2/data/archive.zip\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "OQBIGDoKA9Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-Fae5F708pP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import sys\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "49d0yLVA2vEv"
      },
      "outputs": [],
      "source": [
        "input_shape = (96,96,3)\n",
        "dataset_name = \"stl10\"\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "shuffle_buffer = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "HbBhFd4p1sen"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def data_stl(size = 32):\n",
        "\n",
        "    unlabeled_batch_size = size\n",
        "    labeled_batch_size = size\n",
        "    batch_size = size\n",
        "\n",
        "    train_dataset = (\n",
        "        tfds.load(dataset_name, split=\"train\", as_supervised=True, shuffle_files=True)\n",
        "\n",
        "    )\n",
        "    test_dataset = (\n",
        "        tfds.load(dataset_name, split=\"test\", as_supervised=True)\n",
        "    )\n",
        "\n",
        "    X_train, y_train = tuple(zip(*train_dataset))\n",
        "\n",
        "    X_test, y_test = tuple(zip(*test_dataset))\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    y_train2 = np.zeros((y_train.size, y_train.max()+1), dtype=int)\n",
        "    y_train2[np.arange(y_train.size),y_train] = 1\n",
        "\n",
        "    y_test2 = np.zeros((y_test.size, y_test.max()+1), dtype=int)\n",
        "    y_test2[np.arange(y_test.size),y_test] = 1\n",
        "\n",
        "\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((tf.constant(X_train),\n",
        "                                                tf.constant(y_train2)))\n",
        "\n",
        "\n",
        "    train_data = train_data.shuffle(buffer_size=len(X_train))\n",
        "\n",
        "\n",
        "    train_data = train_data.batch(batch_size , drop_remainder=True)\n",
        "\n",
        "\n",
        "\n",
        "    test_data = tf.data.Dataset.from_tensor_slices((tf.constant(X_test),\n",
        "                                                tf.constant(y_test2)))\n",
        "\n",
        "\n",
        "    test_data = test_data.shuffle(buffer_size=len(X_test))\n",
        "\n",
        "\n",
        "    test_data = test_data.batch(batch_size , drop_remainder=True)\n",
        "\n",
        "\n",
        "\n",
        "    return train_data, test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tiny Imagenet\n",
        "\n",
        "def process_image(image_path , IMG_SIZE = 96):\n",
        "\n",
        "  image = tf.io.read_file(image_path)\n",
        "\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def get_image_label(image_path, label):\n",
        "\n",
        "  image = process_image(image_path)\n",
        "  return image, label\n",
        "\n",
        "\n",
        "def data_imagenet(batch_size=32):\n",
        "\n",
        "\n",
        "    base  = \"/content/tiny-imagenet-200/train\"\n",
        "\n",
        "    files = []\n",
        "    labels = []\n",
        "    i = 0\n",
        "    for p in sorted(os.listdir(base)):\n",
        "\n",
        "        base2 =  os.path.join(base,p,\"images\")\n",
        "\n",
        "        t = []\n",
        "        t1 = []\n",
        "        for p2 in sorted(os.listdir(base2)):\n",
        "\n",
        "            t.append(os.path.join(base2,p2))\n",
        "            t1.append(i)\n",
        "\n",
        "        files.append(t)\n",
        "        labels.append(t1)\n",
        "        i+=1\n",
        "\n",
        "\n",
        "\n",
        "    files = np.array(files)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "\n",
        "    files = files.flatten()\n",
        "    labels = labels.flatten()\n",
        "\n",
        "\n",
        "\n",
        "    print(files.shape)\n",
        "    print(labels.shape)\n",
        "\n",
        "\n",
        "    X_train, X_val_, y_train, y_val_ = train_test_split(files,\n",
        "                                                    labels,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    print(\"X_train \", X_train.shape)\n",
        "    print(\"X_val_ \", X_val_.shape)\n",
        "    print(\"y_train \", y_train.shape)\n",
        "    print(\"y_val_ \", y_val_.shape)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val_,\n",
        "                                                    y_val_,\n",
        "                                                    test_size=0.1,\n",
        "                                                    random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    print(\"X_val \", X_val.shape)\n",
        "    print(\"X_test \", X_test.shape)\n",
        "    print(\"y_val \", y_val.shape)\n",
        "    print(\"y_test \", y_test.shape)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    print(\"X_train \", X_train.shape)\n",
        "    print(\"y_train \", y_train.shape)\n",
        "    print(\"X_val \", X_val.shape)\n",
        "    print(\"y_val \", y_val.shape)\n",
        "\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "\n",
        "    AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "\n",
        "    y_val = tf.constant(y_val)\n",
        "    depth = int(tf.reduce_max(y_val))+1\n",
        "    y_val = tf.one_hot(y_val , depth)\n",
        "\n",
        "\n",
        "    y_test = tf.constant(y_test)\n",
        "    y_test = tf.one_hot(y_test , depth)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    val_data = tf.data.Dataset.from_tensor_slices((tf.constant(X_val),\n",
        "                                            y_val))\n",
        "\n",
        "\n",
        "    val_data = val_data.shuffle(buffer_size=len(X_val))\n",
        "\n",
        "\n",
        "    val_data = val_data.map(get_image_label).batch(batch_size , drop_remainder=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_data = tf.data.Dataset.from_tensor_slices((tf.constant(X_test),\n",
        "                                            y_test))\n",
        "\n",
        "\n",
        "    test_data = test_data.shuffle(buffer_size=len(X_test))\n",
        "\n",
        "\n",
        "    test_data = test_data.map(get_image_label).batch(batch_size , drop_remainder=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    val_data = val_data.prefetch(buffer_size=AUTO)\n",
        "\n",
        "    test_data = test_data.prefetch(buffer_size=AUTO)\n",
        "\n",
        "\n",
        "    return (val_data , test_data )\n",
        "\n"
      ],
      "metadata": {
        "id": "OB2TVrg_93oc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cifar-10\n",
        "\n",
        "\n",
        "def crop(image,mask , CROP_TO = 96 ):\n",
        "    # With random crops we also apply horizontal flipping.\n",
        "    # image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.resize(image, size = [CROP_TO, CROP_TO])\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "\n",
        "def data_cifar(size = 32 , CROP_TO = 96,name = \"cifar10\"):\n",
        "\n",
        "    if(name == \"cifar10\"):\n",
        "\n",
        "        (Xc_train, y_train), (Xc_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "    else:\n",
        "\n",
        "        (Xc_train, y_train), (Xc_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
        "\n",
        "    y_train = np.squeeze(y_train, axis=1)\n",
        "\n",
        "    y_test = np.squeeze(y_test, axis=1)\n",
        "\n",
        "\n",
        "    print(\" Xc_train   yf_train \" , Xc_train.shape , y_train.shape)\n",
        "    print(\" Xc_test yv_test\" ,  Xc_test.shape ,  y_test.shape)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    SEED = 42\n",
        "\n",
        "\n",
        "\n",
        "    AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "\n",
        "    y_train = tf.constant(y_train)\n",
        "    depth = int(tf.reduce_max(y_train))+1\n",
        "    y_train = tf.one_hot(y_train , depth)\n",
        "\n",
        "\n",
        "\n",
        "    y_test = tf.constant(y_test)\n",
        "    y_test = tf.one_hot(y_test , depth)\n",
        "\n",
        "\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((tf.constant(Xc_train), y_train))\n",
        "\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((tf.constant(Xc_test), y_test))\n",
        "\n",
        "\n",
        "\n",
        "    train_ds = train_ds.map(crop).batch(size).prefetch(AUTO)\n",
        "\n",
        "\n",
        "    test_ds = test_ds.map(crop).batch(size).prefetch(AUTO)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"size -- \" , size)\n",
        "\n",
        "    print(\"train_ds --  \" , train_ds)\n",
        "\n",
        "\n",
        "    print(\"test_ds-- \" , test_ds)\n",
        "\n",
        "    return (train_ds , test_ds )\n"
      ],
      "metadata": {
        "id": "LC0FBJFpEsbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "j1tdXd_u1ZB9"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RandomResizedCrop(layers.Layer):\n",
        "    def __init__(self, scale, ratio):\n",
        "        super(RandomResizedCrop, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.log_ratio = (tf.math.log(ratio[0]), tf.math.log(ratio[1]))\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        height = tf.shape(images)[1]\n",
        "        PROJECT_DIM = tf.shape(images)[2]\n",
        "\n",
        "        random_scales = tf.random.uniform((batch_size,), self.scale[0], self.scale[1])\n",
        "        random_ratios = tf.exp(\n",
        "            tf.random.uniform((batch_size,), self.log_ratio[0], self.log_ratio[1])\n",
        "        )\n",
        "\n",
        "        new_heights = tf.clip_by_value(tf.sqrt(random_scales / random_ratios), 0, 1)\n",
        "        new_PROJECT_DIMs = tf.clip_by_value(tf.sqrt(random_scales * random_ratios), 0, 1)\n",
        "        height_offsets = tf.random.uniform((batch_size,), 0, 1 - new_heights)\n",
        "        PROJECT_DIM_offsets = tf.random.uniform((batch_size,), 0, 1 - new_PROJECT_DIMs)\n",
        "\n",
        "        bounding_boxes = tf.stack(\n",
        "            [\n",
        "                height_offsets,\n",
        "                PROJECT_DIM_offsets,\n",
        "                height_offsets + new_heights,\n",
        "                PROJECT_DIM_offsets + new_PROJECT_DIMs,\n",
        "            ],\n",
        "            axis=1,\n",
        "        )\n",
        "        images = tf.image.crop_and_resize(\n",
        "            images, bounding_boxes, tf.range(batch_size), (height, PROJECT_DIM)\n",
        "        )\n",
        "        return images\n",
        "\n",
        "\n",
        "\n",
        "class RandomBrightness(layers.Layer):\n",
        "    def __init__(self, brightness):\n",
        "        super(RandomBrightness, self).__init__()\n",
        "        self.brightness = brightness\n",
        "\n",
        "    def blend(self, images_1, images_2, ratios):\n",
        "        return tf.clip_by_value(ratios * images_1 + (1.0 - ratios) * images_2, 0, 1)\n",
        "\n",
        "    def random_brightness(self, images):\n",
        "        # random interpolation/extrapolation between the image and darkness\n",
        "        return self.blend(\n",
        "            images,\n",
        "            0,\n",
        "            tf.random.uniform(\n",
        "                (tf.shape(images)[0], 1, 1, 1), 1 - self.brightness, 1 + self.brightness\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def call(self, images):\n",
        "        images = self.random_brightness(images)\n",
        "        return images\n",
        "\n",
        "def augmenter(brightness, name, scale):\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=input_shape),\n",
        "            layers.Rescaling(1 / 255),\n",
        "            layers.RandomFlip(\"horizontal\"),\n",
        "            RandomResizedCrop(scale=scale, ratio=(3 / 4, 4 / 3)),\n",
        "            RandomBrightness(brightness=brightness),\n",
        "        ],\n",
        "        name=name,\n",
        "    )\n",
        "\n",
        "\n",
        "def augmenter2( brightness, name, scale):\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=input_shape),\n",
        "            layers.Rescaling(1 / 255),\n",
        "            layers.RandomFlip(\"horizontal\"),\n",
        "            layers.RandomZoom(0.2),\n",
        "            layers.RandomRotation(0.3),\n",
        "            layers.RandomContrast(0.2),\n",
        "            layers.RandomHeight(0.2),\n",
        "\n",
        "            RandomResizedCrop(scale=scale, ratio=(3 / 4, 4 / 3)),\n",
        "            RandomBrightness(brightness=brightness),\n",
        "        ],\n",
        "        name=name,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "I7l39E-i4zXw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evalute_method(model, val, test,n , l =  0.001,l1 =  0.001,epochs = 50 ,size = 32, input_shape = (96,96,3),r = False):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "    print(\"______________________________\")\n",
        "    print(\"Hyper parameter...\")\n",
        "    print(\" n = \", n)\n",
        "    print(\" l = \",  l)\n",
        "    print(\" l1 = \", l1)\n",
        "    print(\" epochs = \", epochs)\n",
        "    print(\" size = \", size)\n",
        "    print(\" input_shape = \", input_shape )\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "\n",
        "    finetuning_model = keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=input_shape),\n",
        "            augmenter( brightness = 0.2, name =  'contrastive_augmenter', scale =  (0.5, 1.0)),\n",
        "            model,\n",
        "            layers.Dense(n  ,activation = \"softmax\"),\n",
        "        ],\n",
        "        name=\"finetuning_model\",\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    finetuning_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(l),\n",
        "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.CategoricalAccuracy(name=\"Accuracy\") ,\n",
        "                tfa.metrics.F1Score(num_classes = n ,  name='F1_score'),\n",
        "                    tf.keras.metrics.Precision(name = \"Precision\"),\n",
        "                    tfa.metrics.RSquare(name = \"R2 score\"),\n",
        "                    tf.keras.metrics.Recall(name = \"Recall\"),\n",
        "                    tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='Top 5 Accuracy'),\n",
        "                    tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='Top 3 Accuracy')\n",
        "\n",
        "                    ],\n",
        "    )\n",
        "\n",
        "    history = finetuning_model.fit(\n",
        "        val, epochs=epochs, validation_data=test\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # res = finetuning_model.evaluate(test)\n",
        "\n",
        "    df = history.history\n",
        "\n",
        "    avg_train_F1 = [ sum(i)/len(i) for i in  df[\"F1_score\"]]\n",
        "    avg_val_F1  = [ sum(i)/len(i) for i in  df[\"val_F1_score\"]]\n",
        "\n",
        "    df[\"F1_score\"] = avg_train_F1\n",
        "    df[\"val_F1_score\"] = avg_val_F1\n",
        "\n",
        "    if(r):\n",
        "\n",
        "        return finetuning_model, df\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "T8i9h-sU3nJ2"
      },
      "outputs": [],
      "source": [
        "def get_encoder2():\n",
        "    base_model = tf.keras.applications.ResNet50(include_top=False,\n",
        "        weights=\"imagenet\", input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "\n",
        "    inputs = tf.keras.layers.Input(input_shape)\n",
        "    x = base_model(inputs, training=True)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dense(2048, activation='relu', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    z = tf.keras.layers.Dense(2048)(x)\n",
        "\n",
        "    f = tf.keras.Model(inputs, z)\n",
        "\n",
        "    return f\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_imagenet , test_imagenet = data_imagenet()\n",
        "train_stl , test_stl  = data_stl()"
      ],
      "metadata": {
        "id": "AB0cMsKhBU5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds1vhseD3UHO"
      },
      "outputs": [],
      "source": [
        "model_ =  get_encoder2()\n",
        "\n",
        "model_.load_weights('/content/drive/MyDrive/self_supervised2/model_pNNCLR_WOB_mark-x_imgnet.h5')\n",
        "\n",
        "model_ = tf.keras.Model(model_.input, model_.layers[2].output)\n",
        "\n",
        "model_.trainable = False\n",
        "\n",
        "df1 = evalute_method(model_,train_stl , test_stl , n = 10 )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}