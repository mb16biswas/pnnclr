{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XrQd3L-tpns"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !pip install  tensorflow==2.8\n",
        "!git clone https://github.com/beresandras/contrastive-classification-keras.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BJ0UPCe1KTA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import sys\n",
        "sys.path.insert(0,'/content/contrastive-classification-keras')\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from algorithms import SimCLR, NNCLR, DCCLR, BarlowTwins, HSICTwins, TWIST, MoCo, DINO\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHL9GVyu2KzW"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "epochs = 1\n",
        "steps_per_epoch = 200\n",
        "PROJECT_DIM = 128\n",
        "\n",
        "input_shape = (96,96,3)\n",
        "# hyperparameters corresponding to each algorithm\n",
        "hyperparams = {\n",
        "    \"SimCLR\": {\"temperature\": 0.1},\n",
        "    \"NNCLR\": {\"temperature\": 0.1, \"queue_size\": 10000},\n",
        "    \"DCCLR\": {\"temperature\": 0.1},\n",
        "    \"BarlowTwins\": {\"redundancy_reduction_weight\": 10.0},\n",
        "    \"HSICTwins\": {\"redundancy_reduction_weight\": 3.0},\n",
        "    \"TWIST\": {},\n",
        "    \"MoCo\": {\"momentum_coeff\": 0.99, \"temperature\": 0.1, \"queue_size\": 10000},\n",
        "    \"DINO\": {\"momentum_coeff\": 0.9, \"temperature\": 0.1, \"sharpening\": 0.5},\n",
        "}\n",
        "\n",
        "ssl = {\n",
        "    \"SimCLR\" : SimCLR,\n",
        "    \"NNCLR\" : NNCLR,\n",
        "    \"DCCLR\" : DCCLR,\n",
        "    \"BarlowTwins\" : BarlowTwins,\n",
        "    \"HSICTwins\" : HSICTwins,\n",
        "    \"TWIST\" : TWIST ,\n",
        "    \"MoCo\" : MoCo,\n",
        "    \"DINO\" : DINO\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "temperature = 0.1\n",
        "queue_size = 10000\n",
        "\n",
        "\n",
        "\n",
        "classification_augmenter = {\n",
        "    \"brightness\": 0.2,\n",
        "    \"name\": \"classification_augmenter\",\n",
        "    \"scale\": (0.5, 1.0),\n",
        "}\n",
        "\n",
        "contrastive_augmenter = {\n",
        "    \"brightness\": 0.5,\n",
        "    \"name\": \"contrastive_augmenter\",\n",
        "    \"scale\": (0.2, 1.0),\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "shuffle_buffer = 5000\n",
        "# The below two values are taken from https://www.tensorflow.org/datasets/catalog/stl10\n",
        "labelled_train_images = 5000\n",
        "unlabelled_images = 100000\n",
        "\n",
        "PROJECT_DIM = 2048\n",
        "WEIGHT_DECAY = 0.0005\n",
        "LATENT_DIM = 512\n",
        "CROP_TO = 96\n",
        "batch_size = 32\n",
        "l = 0.001\n",
        "dataset_name = \"stl10\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMaOKDN3-_Zg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def prepare_dataset_stl(size = 32):\n",
        "\n",
        "    unlabeled_batch_size = size\n",
        "    labeled_batch_size = size\n",
        "    batch_size = size\n",
        "\n",
        "    # unlabeled_batch_size = 500\n",
        "    # labeled_batch_size = 500\n",
        "    # batch_size = 500\n",
        "\n",
        "\n",
        "    unlabeled_train_dataset = (\n",
        "        tfds.load(\n",
        "            dataset_name, split=\"unlabelled\", as_supervised=True, shuffle_files=True\n",
        "        )\n",
        "        .shuffle(buffer_size=shuffle_buffer)\n",
        "        .batch(unlabeled_batch_size, drop_remainder=True)\n",
        "    )\n",
        "    labeled_train_dataset = (\n",
        "        tfds.load(dataset_name, split=\"train\", as_supervised=True, shuffle_files=True)\n",
        "        .shuffle(buffer_size=shuffle_buffer)\n",
        "        .batch(labeled_batch_size, drop_remainder=True)\n",
        "    )\n",
        "    test_dataset = (\n",
        "        tfds.load(dataset_name, split=\"test\", as_supervised=True)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(buffer_size=AUTOTUNE)\n",
        "    )\n",
        "    train_dataset = tf.data.Dataset.zip(\n",
        "        (unlabeled_train_dataset, labeled_train_dataset)\n",
        "    ).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    return (train_dataset, labeled_train_dataset, test_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtRrpbYyeuVU"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset, labeled_train_dataset, test_dataset = prepare_dataset_stl(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6chIkPx2Uba"
      },
      "outputs": [],
      "source": [
        "class RandomResizedCrop(layers.Layer):\n",
        "    def __init__(self, scale, ratio):\n",
        "        super(RandomResizedCrop, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.log_ratio = (tf.math.log(ratio[0]), tf.math.log(ratio[1]))\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        height = tf.shape(images)[1]\n",
        "        PROJECT_DIM = tf.shape(images)[2]\n",
        "\n",
        "        random_scales = tf.random.uniform((batch_size,), self.scale[0], self.scale[1])\n",
        "        random_ratios = tf.exp(\n",
        "            tf.random.uniform((batch_size,), self.log_ratio[0], self.log_ratio[1])\n",
        "        )\n",
        "\n",
        "        new_heights = tf.clip_by_value(tf.sqrt(random_scales / random_ratios), 0, 1)\n",
        "        new_PROJECT_DIMs = tf.clip_by_value(tf.sqrt(random_scales * random_ratios), 0, 1)\n",
        "        height_offsets = tf.random.uniform((batch_size,), 0, 1 - new_heights)\n",
        "        PROJECT_DIM_offsets = tf.random.uniform((batch_size,), 0, 1 - new_PROJECT_DIMs)\n",
        "\n",
        "        bounding_boxes = tf.stack(\n",
        "            [\n",
        "                height_offsets,\n",
        "                PROJECT_DIM_offsets,\n",
        "                height_offsets + new_heights,\n",
        "                PROJECT_DIM_offsets + new_PROJECT_DIMs,\n",
        "            ],\n",
        "            axis=1,\n",
        "        )\n",
        "        images = tf.image.crop_and_resize(\n",
        "            images, bounding_boxes, tf.range(batch_size), (height, PROJECT_DIM)\n",
        "        )\n",
        "        return images\n",
        "\n",
        "\n",
        "\n",
        "class RandomBrightness(layers.Layer):\n",
        "    def __init__(self, brightness):\n",
        "        super(RandomBrightness, self).__init__()\n",
        "        self.brightness = brightness\n",
        "\n",
        "    def blend(self, images_1, images_2, ratios):\n",
        "        return tf.clip_by_value(ratios * images_1 + (1.0 - ratios) * images_2, 0, 1)\n",
        "\n",
        "    def random_brightness(self, images):\n",
        "        # random interpolation/extrapolation between the image and darkness\n",
        "        return self.blend(\n",
        "            images,\n",
        "            0,\n",
        "            tf.random.uniform(\n",
        "                (tf.shape(images)[0], 1, 1, 1), 1 - self.brightness, 1 + self.brightness\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def call(self, images):\n",
        "        images = self.random_brightness(images)\n",
        "        return images\n",
        "\n",
        "def augmenter(brightness, name, scale):\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=input_shape),\n",
        "            layers.Rescaling(1 / 255),\n",
        "            layers.RandomFlip(\"horizontal\"),\n",
        "            RandomResizedCrop(scale=scale, ratio=(3 / 4, 4 / 3)),\n",
        "            RandomBrightness(brightness=brightness),\n",
        "        ],\n",
        "        name=name,\n",
        "    )\n",
        "\n",
        "\n",
        "def augmenter2( brightness, name, scale):\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=input_shape),\n",
        "            layers.Rescaling(1 / 255),\n",
        "            layers.RandomFlip(\"horizontal\"),\n",
        "            layers.RandomZoom(0.2),\n",
        "            layers.RandomRotation(0.3),\n",
        "            layers.RandomContrast(0.2),\n",
        "            layers.RandomHeight(0.2),\n",
        "\n",
        "            RandomResizedCrop(scale=scale, ratio=(3 / 4, 4 / 3)),\n",
        "            RandomBrightness(brightness=brightness),\n",
        "        ],\n",
        "        name=name,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_encoder2():\n",
        "    base_model = tf.keras.applications.ResNet50(include_top=False,\n",
        "        weights=\"imagenet\", input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "\n",
        "    inputs = layers.Input(input_shape)\n",
        "    x = base_model(inputs, training=True)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(2048, activation='relu', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    z = layers.Dense(2048)(x)\n",
        "\n",
        "    f = tf.keras.Model(inputs, z)\n",
        "\n",
        "    return f\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ln6XiD4Ju9"
      },
      "outputs": [],
      "source": [
        "class PNNCLR(keras.Model):\n",
        "    def __init__(\n",
        "        self, temperature, queue_size,t = 0.99, frac = 0.05 , points = 21, initial = 0.05 , noise_val = 0.1, noise = False, nn = True\n",
        "    ):\n",
        "        super(PNNCLR, self).__init__()\n",
        "        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
        "        self.correlation_accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
        "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
        "        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        self.contrastive_augmenter = augmenter2(**contrastive_augmenter)\n",
        "        self.classification_augmenter = augmenter2(**classification_augmenter)\n",
        "\n",
        "\n",
        "        self.f_online  = get_encoder2()\n",
        "        self.f_target = get_encoder2()\n",
        "        self.g_online = keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=(PROJECT_DIM,)),\n",
        "            layers.Dense(PROJECT_DIM, activation=\"relu\"),\n",
        "            layers.Dense(PROJECT_DIM),\n",
        "        ],\n",
        "            name=\"projection_head\",\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        self.q_online = keras.Sequential(\n",
        "         [\n",
        "            layers.Input(shape=(PROJECT_DIM,)),\n",
        "            layers.Dense(PROJECT_DIM, activation=\"relu\"),\n",
        "            layers.Dense(PROJECT_DIM),\n",
        "        ],\n",
        "            name=\"projection_head1\",\n",
        "\n",
        "        )\n",
        "\n",
        "        self.g_target = keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=(PROJECT_DIM,)),\n",
        "            layers.Dense(PROJECT_DIM, activation=\"relu\"),\n",
        "            layers.Dense(PROJECT_DIM),\n",
        "        ],\n",
        "            name=\"projection_head2\",\n",
        "\n",
        "\n",
        "        )\n",
        "        self.linear_probe = keras.Sequential(\n",
        "            [layers.Input(shape=(PROJECT_DIM,)), layers.Dense(10)], name=\"linear_probe\"\n",
        "        )\n",
        "        self.temperature = temperature\n",
        "\n",
        "        feature_dimensions = self.f_online.output_shape[1]\n",
        "        self.feature_queue = tf.Variable(\n",
        "            tf.math.l2_normalize(\n",
        "                tf.random.normal(shape=(queue_size, feature_dimensions)), axis=1\n",
        "            ),\n",
        "            trainable=False,\n",
        "        )\n",
        "\n",
        "        self.t = t\n",
        "        self.noise = noise\n",
        "        self.nn = nn\n",
        "        self.frac = frac\n",
        "        self.points = points\n",
        "        self.initial = initial\n",
        "        self.noise_val = noise_val\n",
        "        self.index = int(points - frac/initial - 1)\n",
        "\n",
        "\n",
        "        print()\n",
        "        print()\n",
        "        print(\"____config________\")\n",
        "        print(\"frac \" , self.frac)\n",
        "        print(\"points \", self.points)\n",
        "        print(\"initial \", self.initial)\n",
        "        print(\"nn \", self.nn)\n",
        "        print(\"noise \", self.noise)\n",
        "        print(\"noise val\" , self.noise_val)\n",
        "        print(\"t \",self.t)\n",
        "        print(\"index \", self.index)\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "\n",
        "    def gaussian_noise_layer(input_layer, std = 0.2):\n",
        "        noise = tf.random.normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32)\n",
        "        return input_layer + noise\n",
        "\n",
        "    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n",
        "        super(PNNCLR, self).compile(**kwargs)\n",
        "        self.contrastive_optimizer = contrastive_optimizer\n",
        "        self.probe_optimizer = probe_optimizer\n",
        "\n",
        "\n",
        "    def nearest_neighbour(self, projection):\n",
        "\n",
        "        \"\"\"\n",
        "                    projection\n",
        "                    Tensor(\"l2_normalize:0\", shape=(64, 2048), dtype=float32)\n",
        "\n",
        "                    self.feature_queue\n",
        "                    <tf.Variable 'Variable:0' shape=(10000, 2048) dtype=float32>\n",
        "\n",
        "                    support_similarities\n",
        "                    Tensor(\"MatMul:0\", shape=(64, 10000), dtype=float32)\n",
        "\n",
        "                    tf.argmax(support_similarities, axis=1)\n",
        "                    Tensor(\"ArgMax:0\", shape=(64,), dtype=int64)\n",
        "                    nn_projections\n",
        "\n",
        "                    Tensor(\"Identity:0\", shape=(64, 2048), dtype=float32)\n",
        "\n",
        "                    projection + tf.stop_gradient(nn_projections - projections)\n",
        "                    Tensor(\"add:0\", shape=(64, 2048), dtype=float32)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        support_similarities = tf.matmul(\n",
        "            projection, self.feature_queue, transpose_b=True\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        nn_projection = tf.gather(\n",
        "            self.feature_queue, tf.argmax(support_similarities, axis=1), axis=0\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        if(self.nn):\n",
        "\n",
        "\n",
        "            # nn_projections = tf.keras.layers.GaussianNoise(0.1)(nn_projections)\n",
        "\n",
        "            # new_projections =  (self.a) * nn_projections + (self.b) * projections\n",
        "\n",
        "            new_projections = tf.linspace(projection, nn_projection , num=self.points)\n",
        "\n",
        "\n",
        "            new_projection = new_projections[self.index]\n",
        "\n",
        "\n",
        "\n",
        "            if(self.noise):\n",
        "\n",
        "                new_projection = tf.keras.layers.GaussianNoise(self.noise_val)(new_projection)\n",
        "\n",
        "            return new_projection\n",
        "\n",
        "        else:\n",
        "\n",
        "            return nn_projection\n",
        "\n",
        "\n",
        "    def update_contrastive_accuracy(self, features_1, features_2):\n",
        "        features_1 = tf.math.l2_normalize(features_1, axis=1)\n",
        "        features_2 = tf.math.l2_normalize(features_2, axis=1)\n",
        "        similarities = tf.matmul(features_1, features_2, transpose_b=True)\n",
        "\n",
        "        batch_size = tf.shape(features_1)[0]\n",
        "        contrastive_labels = tf.range(batch_size)\n",
        "        self.contrastive_accuracy.update_state(\n",
        "            tf.concat([contrastive_labels, contrastive_labels], axis=0),\n",
        "            tf.concat([similarities, tf.transpose(similarities)], axis=0),\n",
        "        )\n",
        "\n",
        "    def update_correlation_accuracy(self, features_1, features_2):\n",
        "        features_1 = (\n",
        "            features_1 - tf.reduce_mean(features_1, axis=0)\n",
        "        ) / tf.math.reduce_std(features_1, axis=0)\n",
        "        features_2 = (\n",
        "            features_2 - tf.reduce_mean(features_2, axis=0)\n",
        "        ) / tf.math.reduce_std(features_2, axis=0)\n",
        "\n",
        "        batch_size = tf.shape(features_1, out_type=tf.float32)[0]\n",
        "        cross_correlation = (\n",
        "            tf.matmul(features_1, features_2, transpose_a=True) / batch_size\n",
        "        )\n",
        "\n",
        "        feature_dim = tf.shape(features_1)[1]\n",
        "        correlation_labels = tf.range(feature_dim)\n",
        "        self.correlation_accuracy.update_state(\n",
        "            tf.concat([correlation_labels, correlation_labels], axis=0),\n",
        "            tf.concat([cross_correlation, tf.transpose(cross_correlation)], axis=0),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def contrastive_loss(self, projections_1, projections_2):\n",
        "\n",
        "        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n",
        "        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n",
        "\n",
        "        similarities_1_2_1 = (\n",
        "            tf.matmul(\n",
        "                self.nearest_neighbour(projections_1), projections_2, transpose_b=True\n",
        "            )\n",
        "            / self.temperature\n",
        "        )\n",
        "        similarities_1_2_2 = (\n",
        "            tf.matmul(\n",
        "                projections_2, self.nearest_neighbour(projections_1), transpose_b=True\n",
        "            )\n",
        "            / self.temperature\n",
        "        )\n",
        "\n",
        "        similarities_2_1_1 = (\n",
        "            tf.matmul(\n",
        "                self.nearest_neighbour(projections_2), projections_1, transpose_b=True\n",
        "            )\n",
        "            / self.temperature\n",
        "        )\n",
        "        similarities_2_1_2 = (\n",
        "            tf.matmul(\n",
        "                projections_1, self.nearest_neighbour(projections_2), transpose_b=True\n",
        "            )\n",
        "            / self.temperature\n",
        "        )\n",
        "\n",
        "        batch_size = tf.shape(projections_1)[0]\n",
        "        contrastive_labels = tf.range(batch_size)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(\n",
        "            tf.concat(\n",
        "                [\n",
        "                    contrastive_labels,\n",
        "                    contrastive_labels,\n",
        "                    contrastive_labels,\n",
        "                    contrastive_labels,\n",
        "                ],\n",
        "                axis=0,\n",
        "            ),\n",
        "            tf.concat(\n",
        "                [\n",
        "                    similarities_1_2_1,\n",
        "                    similarities_1_2_2,\n",
        "                    similarities_2_1_1,\n",
        "                    similarities_2_1_2,\n",
        "                ],\n",
        "                axis=0,\n",
        "            ),\n",
        "            from_logits=True,\n",
        "        )\n",
        "\n",
        "        self.feature_queue.assign(\n",
        "            tf.concat([projections_1, self.feature_queue[:-batch_size]], axis=0)\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train_step(self, data):\n",
        "\n",
        "        (unlabeled_images, _), (labeled_images, labels) = data\n",
        "        images = tf.concat((unlabeled_images, labeled_images), axis=0)\n",
        "\n",
        "        x1 = self.contrastive_augmenter(images)\n",
        "        x2 = self.contrastive_augmenter(images)\n",
        "\n",
        "        h_target_1 = self.f_target(x1)\n",
        "        z_target_1 = self.g_target(h_target_1)\n",
        "\n",
        "        h_target_2 = self.f_target(x2)\n",
        "        z_target_2 = self.g_target(h_target_2)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            h_online_1 = self.f_online(x1)\n",
        "            z_online_1  = self.g_online(h_online_1)\n",
        "            p_online_1 = self.q_online(z_online_1)\n",
        "\n",
        "            h_online_2 = self.f_online(x2)\n",
        "            z_online_2  = self.g_online(h_online_2)\n",
        "            p_online_2 = self.q_online(z_online_2)\n",
        "\n",
        "\n",
        "            loss = self.contrastive_loss(p_online_1, z_target_2)/2 + self.contrastive_loss(p_online_2, z_target_1)/2\n",
        "\n",
        "        # Backward pass (update online networks)\n",
        "        grads = tape.gradient(loss, self.f_online.trainable_weights )\n",
        "        self.contrastive_optimizer.apply_gradients(zip(grads, self.f_online.trainable_weights ))\n",
        "        grads = tape.gradient(loss, self.g_online.trainable_weights )\n",
        "        self.contrastive_optimizer.apply_gradients(zip(grads, self.g_online.trainable_weights ))\n",
        "        grads = tape.gradient(loss, self.q_online.trainable_weights )\n",
        "        self.contrastive_optimizer.apply_gradients(zip(grads, self.q_online.trainable_weights ))\n",
        "\n",
        "        del tape\n",
        "\n",
        "\n",
        "\n",
        "        self.update_contrastive_accuracy(h_online_1, h_online_2)\n",
        "        self.update_correlation_accuracy(h_online_1, h_online_2)\n",
        "        preprocessed_images = self.classification_augmenter(labeled_images)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            features = self.f_online(preprocessed_images)\n",
        "            class_logits = self.linear_probe(features)\n",
        "            probe_loss = self.probe_loss(labels, class_logits)\n",
        "        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n",
        "        self.probe_optimizer.apply_gradients(\n",
        "            zip(gradients, self.linear_probe.trainable_weights)\n",
        "        )\n",
        "        self.probe_accuracy.update_state(labels, class_logits)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       # the momentum networks are updated by exponential moving average\n",
        "        for weight, m_weight in zip(self.f_online.weights, self.f_target.weights):\n",
        "            m_weight.assign(\n",
        "               self.t * m_weight + (1 - self.t) * weight\n",
        "            )\n",
        "        for weight, m_weight in zip(\n",
        "            self.g_online.weights, self.g_target.weights\n",
        "        ):\n",
        "            m_weight.assign(\n",
        "                self.t * m_weight + (1 - self.t) * weight\n",
        "            )\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"c_loss\": loss,\n",
        "            \"c_acc\": self.contrastive_accuracy.result(),\n",
        "            \"r_acc\": self.correlation_accuracy.result(),\n",
        "            \"p_loss\": probe_loss,\n",
        "            \"p_acc\": self.probe_accuracy.result(),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def test_step(self, data):\n",
        "        labeled_images, labels = data\n",
        "\n",
        "        preprocessed_images = self.classification_augmenter(\n",
        "            labeled_images, training=False\n",
        "        )\n",
        "        features = self.f_online(preprocessed_images, training=False)\n",
        "        class_logits = self.linear_probe(features, training=False)\n",
        "        probe_loss = self.probe_loss(labels, class_logits)\n",
        "\n",
        "        self.probe_accuracy.update_state(labels, class_logits)\n",
        "        return {\"p_loss\": probe_loss, \"p_acc\": self.probe_accuracy.result()}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkCtvSp25S7Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ssl_method(name = \"SimCLR\"):\n",
        "\n",
        "\n",
        "    Algorithm = ssl[name]\n",
        "\n",
        "    # architecture\n",
        "    model_ = Algorithm(\n",
        "    contrastive_augmenter= augmenter2(**contrastive_augmenter),\n",
        "    classification_augmenter= augmenter2(**classification_augmenter),\n",
        "    encoder= get_encoder2() ,\n",
        "    projection_head=keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=(PROJECT_DIM,)),\n",
        "            layers.Dense(PROJECT_DIM, activation=\"relu\"),\n",
        "            layers.Dense(PROJECT_DIM),\n",
        "        ],\n",
        "        name=\"projection_head\",\n",
        "    ),\n",
        "    linear_probe=keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=(PROJECT_DIM,)),\n",
        "            layers.Dense(10),\n",
        "        ],\n",
        "        name=\"linear_probe\",\n",
        "    ),\n",
        "    **hyperparams[name],\n",
        "    )\n",
        "\n",
        "\n",
        "    return model_\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik3x3Y7j8pV3"
      },
      "outputs": [],
      "source": [
        "def train_ssl(model,train_ssl,val_data):\n",
        "\n",
        "\n",
        "    steps = epochs * (5000// batch_size)\n",
        "    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate= l , decay_steps=steps)\n",
        "\n",
        "    model.compile(\n",
        "        contrastive_optimizer= tf.keras.optimizers.legacy.SGD(lr_decayed_fn, momentum=0.9),\n",
        "        probe_optimizer= tf.keras.optimizers.legacy.SGD(lr_decayed_fn, momentum=0.9),\n",
        "    )\n",
        "\n",
        "    # run training\n",
        "    history = model.fit(train_ssl, epochs=epochs, validation_data=val_data )\n",
        "\n",
        "    return history, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll8WOFD19P9F"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model_SimCLR =  ssl_method(name = \"SimCLR\")\n",
        "history ,model_SimCLR = train_ssl(model_SimCLR,train_dataset,test_dataset)\n",
        "# model_SimCLR.encoder.save_weights('required path')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_NNCLR=  ssl_method(name = \"NNCLR\")\n",
        "history ,model_NNCLR = train_ssl(model_NNCLR,train_dataset,test_dataset)\n",
        "# model_NNCLR.encoder.save_weights('required path')\n"
      ],
      "metadata": {
        "id": "HkYBH10240Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_PNNCLR = PNNCLR(temperature=temperature, queue_size=queue_size)\n",
        "\n",
        "\n",
        "history ,model_PNNCLR = train_ssl(model_PNNCLR,train_dataset,test_dataset)\n",
        "# model_PNNCLR.f_online.save_weights('required path')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZ8j-AJ7pjF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}